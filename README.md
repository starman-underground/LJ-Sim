# LJ-Sim: Lennard-Jones Molecular Dynamics Simulation

[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Language](https://img.shields.io/badge/language-C%2B%2B-blue.svg)](https://github.com/starman-underground/LJ-Sim)
[![CUDA](https://img.shields.io/badge/CUDA-12.9-green.svg)](https://developer.nvidia.com/cuda-toolkit)
[![Build Status](https://img.shields.io/badge/build-passing-brightgreen.svg)](https://github.com/starman-underground/LJ-Sim)

> **A comprehensive 4-week implementation of GPU-accelerated Lennard-Jones molecular dynamics simulation from first principles**

## üéØ Project Overview

LJ-Sim is a high-performance molecular dynamics simulation engine implementing the Lennard-Jones potential with progressive CUDA optimization. This project demonstrates a systematic approach to building computational physics software, progressing from foundational 2D implementations to GPU-accelerated multi-particle systems capable of thermodynamic analysis.

**üî¨ Scientific Objective:** Simulate intermolecular interactions using the Lennard-Jones potential: `V(r) = 4Œµ[(œÉ/r)¬π¬≤ - (œÉ/r)‚Å∂]`

**üíª Technical Objective:** Achieve 10-100x performance improvement through GPU acceleration and algorithmic optimization

## ‚ú® Key Features

- **Advanced Integration**: Velocity Verlet algorithm with energy conservation validation
- **Optimized Data-Structures**: Force cutoff distance with Verlet List of contributing particles and Linked Cell rebuilding
- **Real-Time Visualization**: Animated particle dynamics using Mimir's CUDA/Vulkan integration
- **Thermodynamic Analysis**: Temperature, pressure, and radial distribution function calculations
- **GPU Acceleration**: CUDA-optimized force calculations with memory optimization
- **Scientific Validation**: Literature-verified physical properties and conservation laws

## üèóÔ∏è Implementation Roadmap

### Week 1: Foundation & 2D Implementation
**üéØ Objective:** Establish theoretical understanding and create working 2D Lennard-Jones simulation

- [x] **Theory mastery**: Lennard-Jones potential derivations and force calculations
- [x] **Environment setup**: Development tools and scientific libraries
- [ ] **Core implementation**: 2D particle system with basic MD algorithms
- [ ] **Validation framework**: Energy conservation testing (< 1% drift over 1000 steps)

**üèÜ Milestone:** Validated 2-particle system with correct LJ interactions

### Week 2: Integration & Scaling
**üéØ Objective:** Advanced integration methods and multi-particle scaling

- [ ] **Velocity Verlet algorithm**: High-accuracy numerical integration
- [ ] **System scaling**: Framework supporting 10-50 particles
- [ ] **Real-time visualization**: Animated particle dynamics
- [ ] **Temperature control**: Thermostat implementation (¬±5% accuracy)

**üèÜ Milestone:** Multi-particle simulation with visualization and temperature control

### Week 3: Analysis & Physical Properties
**üéØ Objective:** Thermodynamic analysis and parameter studies

- [ ] **Microscopic thermodynamics**: Temperature and pressure from particle data
- [ ] **Structural analysis**: Radial Distribution Function (RDF) implementation
- [ ] **Parameter studies**: Systematic density/temperature variations
- [ ] **Literature validation**: Comparison with experimental values

**üèÜ Milestone:** Complete analysis toolkit with validated parameter studies

### Week 4: Optimization & GPU Integration
**üéØ Objective:** GPU acceleration using CUDA

- [ ] **Performance profiling**: Bottleneck identification and analysis
- [ ] **CUDA implementation**: GPU-accelerated force calculations
- [ ] **Memory optimization**: Efficient data access patterns
- [ ] **Benchmarking**: Performance comparison and speedup analysis

**üèÜ Milestone:** GPU-accelerated simulation with comprehensive performance metrics

## üõ†Ô∏è Technical Specifications

### Hardware Requirements
- **CPU**: AMD Ryzen 7600X 6-Core Processor (or equivalent)
- **RAM**: 32GB DDR5 (minimum 16GB)
- **GPU**: NVIDIA RTX 5070 (Compute Capability 8.9+)
- **Storage**: 5GB free space for simulation data

### Software Dependencies
```bash
# Core Programming Environment
C++23                    # Primary development language

# Scientific Computing
Eigen >= 3.4.0          # Numerical operations
Mimir >= 0.0.1     # Visualization and plotting

# GPU Computing
CUDA Toolkit 12.9       # GPU acceleration framework

# Development Tools
Git 2.30+               # Version control
CMake 3.20+             # Build system for C++ components
```

### Optional Dependencies
```bash
# Enhanced Visualization
Plotly >= 5.0.0         # Interactive 3D visualizations
Mayavi >= 4.7.0         # Advanced scientific visualization

# Performance Analysis
NVIDIA Nsight Compute   # GPU profiling
AMD uProf               # CPU profiling

# Documentation
Sphinx >= 4.0.0         # Documentation generation
Jupyter >= 1.0.0        # Interactive notebooks
```

## üöÄ Quick Start

### 1. Clone and Setup
```bash
# Clone repository
git clone https://github.com/starman-underground/LJ-Sim
cd lennard-jones-md

# Setup build environment
./scripts/setup_environment.sh

# Build project
cmake -B build
cmake --build build -j
```

### 2. Verify CUDA Installation
```bash
# Check CUDA availability
mkdir tests/build
cd tests/build
cmake ..
cmake --build . --config Release
.\Release\cuda_test.execd 
```

### 3. Run Basic Simulation
```bash
# Week 1: Basic 2D system
./examples/week1_basic_2d/two_particle_system

# Week 2: Multi-particle with visualization
./examples/week2_integration/multi_particle_system

# Week 3: Analysis and RDF
./examples/week3_analysis/rdf_calculation

# Week 4: GPU benchmarking
./examples/week4_optimization/cpu_vs_gpu_benchmark
```

## üìä Performance Benchmarks

### Target Performance Metrics
| Metric | CPU Target | Actual CPU | GPU Target | Actual GPU |
|--------|-------------|------|------------|------------|
| **Force Calculation** | 1.0x baseline | | 10-50x | TBD |
| **Integration Speed** | 1.0x baseline | | 5-20x | TBD |
| **Memory Throughput** | ~10 GB/s | | ~500 GB/s | TBD |
| **Particle Scaling** | O(N¬≤) | | O(N¬≤) optimized | TBD |

### Validation Criteria
- ‚úÖ **Energy Conservation**: <¬±1% over 10,000 timesteps
- ‚úÖ **Temperature Control**: <¬±5% of target value
- ‚è≥ **Physical Accuracy**: Literature-aligned RDF and thermodynamic properties
- ‚è≥ **Computational Efficiency**: Reasonable scaling from 2‚Üí1000+ particles

## üìÅ Project Structure

```
lennard-jones-md/
‚îú‚îÄ‚îÄ CMakeLists.txt
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ theory/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lennard_jones_derivation.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ velocity_verlet.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ thermodynamics.md
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îî‚îÄ‚îÄ examples/
‚îú‚îÄ‚îÄ include/
‚îÇ   ‚îú‚îÄ‚îÄ ljmd/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ particle.hpp
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ system.hpp
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ forces.hpp
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ integrator.hpp
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analysis/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ thermodynamics.hpp
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rdf.hpp
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ statistics.hpp
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ visualization/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ renderer.hpp
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ plotting.hpp
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cuda/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gpu_forces.cuh
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gpu_integrator.cuh
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ memory_manager.cuh
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ constants.hpp
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ vector2d.hpp
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ timer.hpp
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ particle.cpp
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ system.cpp
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ forces.cpp
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ integrator.cpp
‚îÇ   ‚îú‚îÄ‚îÄ analysis/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ thermodynamics.cpp
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rdf.cpp
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ statistics.cpp
‚îÇ   ‚îú‚îÄ‚îÄ visualization/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ renderer.cpp
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ plotting.cpp
‚îÇ   ‚îú‚îÄ‚îÄ cuda/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gpu_forces.cu
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gpu_integrator.cu
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ memory_manager.cu
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ vector2d.cpp
‚îÇ       ‚îî‚îÄ‚îÄ timer.cpp
‚îú‚îÄ‚îÄ examples/
‚îÇ   ‚îú‚îÄ‚îÄ week1_basic_2d/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ two_particle_system.cpp
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ energy_conservation.cpp
‚îÇ   ‚îú‚îÄ‚îÄ week2_integration/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ velocity_verlet_demo.cpp
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ multi_particle_system.cpp
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ temperature_control.cpp
‚îÇ   ‚îú‚îÄ‚îÄ week3_analysis/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ thermodynamic_properties.cpp
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rdf_calculation.cpp
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ parameter_studies.cpp
‚îÇ   ‚îî‚îÄ‚îÄ week4_optimization/
‚îÇ       ‚îú‚îÄ‚îÄ cpu_vs_gpu_benchmark.cpp
‚îÇ       ‚îú‚îÄ‚îÄ memory_optimization.cpp
‚îÇ       ‚îî‚îÄ‚îÄ scaling_analysis.cpp
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ unit_tests/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_particle.cpp
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_forces.cpp
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_integrator.cpp
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_analysis.cpp
‚îÇ   ‚îú‚îÄ‚îÄ integration_tests/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_energy_conservation.cpp
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_temperature_control.cpp
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_gpu_equivalence.cpp
‚îÇ   ‚îî‚îÄ‚îÄ benchmarks/
‚îÇ       ‚îú‚îÄ‚îÄ force_calculation_benchmark.cpp
‚îÇ       ‚îî‚îÄ‚îÄ integration_benchmark.cpp
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ setup_environment.sh
‚îÇ   ‚îú‚îÄ‚îÄ build.sh
‚îÇ   ‚îú‚îÄ‚îÄ run_tests.sh
‚îÇ   ‚îî‚îÄ‚îÄ generate_plots.py
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ input/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ argon_parameters.json
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ initial_conditions.dat
‚îÇ   ‚îî‚îÄ‚îÄ output/
‚îÇ       ‚îú‚îÄ‚îÄ trajectories/
‚îÇ       ‚îú‚îÄ‚îÄ analysis/
‚îÇ       ‚îî‚îÄ‚îÄ plots/
‚îî‚îÄ‚îÄ third_party/
    ‚îú‚îÄ‚îÄ eigen/
    ‚îú‚îÄ‚îÄ mimir/
    ‚îî‚îÄ‚îÄ catch2/
```

## üß™ Usage Examples

### Basic 2D Simulation
```cpp
#include <iostream>
#include <memory>
#include "ljmd/core/system.hpp"
#include "ljmd/core/particle.hpp"
#include "ljmd/core/forces.hpp"
#include "ljmd/core/integrator.hpp"
#include "ljmd/utils/vector2d.hpp"

using namespace ljmd;

int main() {
    // Create system
    System system(10.0, 10.0);
    
    // Create two particles
    auto particle1 = std::make_shared<Particle>(1.0, 1.0, 1.0);
    auto particle2 = std::make_shared<Particle>(1.0, 1.0, 1.0);
    
    // Set initial positions
    particle1->set_position(Vector2D(4.0, 5.0));
    particle2->set_position(Vector2D(6.0, 5.0));
    
    // Set initial velocities
    particle1->set_velocity(Vector2D(0.1, 0.0));
    particle2->set_velocity(Vector2D(-0.1, 0.0));
    
    // Add particles to system
    system.add_particle(particle1);
    system.add_particle(particle2);
    
    // Set up force calculator and integrator
    system.set_force_calculator(std::make_unique<LennardJonesForces>(2.5));
    system.set_integrator(std::make_unique<EulerIntegrator>());
    
    // Run simulation
    const int num_steps = 1000;
    const double dt = 0.001;
    
    std::cout << "Time\tTotal Energy\tKinetic Energy\tPotential Energy\n";
    
    for (int step = 0; step < num_steps; ++step) {
        system.step(dt);
        
        if (step % 50 == 0) {
            double ke = system.total_kinetic_energy();
            double pe = system.total_potential_energy();
            double total = ke + pe;
            
            std::cout << step * dt << "\t" << total << "\t" 
                     << ke << "\t" << pe << "\n";
        }
    }
    
    return 0;
}
```

## ü§ù Contributing

Contributions are welcome! This project serves as both a learning exercise and a community resource for molecular dynamics education.

### Development Workflow
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-optimization`)
3. Commit changes (`git commit -m 'Add GPU memory pooling'`)
4. Push to branch (`git push origin feature/amazing-optimization`)
5. Open a Pull Request

### Code Standards
- **Python**: Follow PEP 8, use type hints
- **C++/CUDA**: Follow Google C++ Style Guide
- **Documentation**: Include docstrings and inline comments
- **Testing**: Add unit tests for new functionality

## üìö References and Resources

### Core Literature
- Carter, F., Hitschfeld, N., and Navarro, C. A. (2025). *Mimir: A real-time interactive visualization library for CUDA programs*. University of Chile.
- Nguyen J. (2018). *Shared-Memory Parallelization of Verlet lists*. Technische Universitat Munchen.
- Rizzi M. and Schmitt M. (2022). *Molecular Dynamics: Lennard-Jones Fluid*. M-LAB, Institute of Theoretical Physics, Cologne

### CUDA Programming
- [CUDA Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)
- [CUDA Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/)
- [Molecular Dynamics GPU Optimization](https://developer.nvidia.com/molecular-dynamics)

### Scientific Software Development
- [Best Practices for Scientific Computing](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1001745)
- [Good Enough Practices](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510)

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üìß Contact

**Project Maintainer**: Sree Kommalapati
- üìß Email: sree.n.kommalapati@gmail.com
- üêô GitHub: [@starman-underground](https://github.com/starman-underground)
- üíº LinkedIn: [sree-kommalapati](https://www.linkedin.com/in/sree-kommalapati/)
<!-- - üåê Website: [yourwebsite.com](https://yourwebsite.com) -->

## üôè Acknowledgments

- **Hardware Support**: NVIDIA RTX 5070 for GPU development
- **Scientific Guidance**: Computational physics and molecular dynamics community
- **Open Source Libraries**: CUDA, Eigen, Mimir

---

‚≠ê **Star this repository if it helps your molecular dynamics journey!**

*Built with ‚ù§Ô∏è for the computational physics community*